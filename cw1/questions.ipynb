{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1: ML basics and fully-connected networks\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Please submit a version of this notebook containing your answers on CATe as *CW1*. Write your answers in the cells below each question.\n",
    "\n",
    "We recommend that you work on the Ubuntu workstations in the lab. This assignment and all code were only tested to work on these machines. In particular, we cannot guarantee compatibility with Windows machines and cannot promise support if you choose to work on a Windows machine.\n",
    "\n",
    "You can work from home and use the lab workstations via ssh (for list of machines: https://www.doc.ic.ac.uk/csg/facilities/lab/workstations). \n",
    "\n",
    "Once logged in, run the following commands in the terminal to set up a Python environment with all the packages you will need.\n",
    "\n",
    "    export PYTHONUSERBASE=/vol/bitbucket/nuric/pypi\n",
    "    export PATH=/vol/bitbucket/nuric/pypi/bin:$PATH\n",
    "\n",
    "Add the above lines to your `.bashrc` to have these enviroment variables set automatically each time you open your bash terminal.\n",
    "\n",
    "Any code that you submit will be expected to run in this environment. Marks will be deducted for code that fails to run.\n",
    "\n",
    "Run `jupyter-notebook` in the coursework directory to launch Jupyter notebook in your default browser.\n",
    "\n",
    "DO NOT attempt to create a virtualenv in your home folder as you will likely exceed your file quota.\n",
    "\n",
    "**DEADLINE: 7pm, Tuesday 5th February, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "1. Describe two practical methods used to estimate a supervised learning model's performance on unseen data. Which strategy is most commonly used in most deep learning applications, and why?\n",
    "2. Suppose that you have reason to believe that your multi-layer fully-connected neural network is overfitting. List four things that you could try to improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 1 IN THIS CELL*\\*\n",
    "\n",
    "# Holdout, Cross-Validation and most commonly used in Deep NNs\n",
    "\n",
    "Two practical methods used to estimate a model's performance are Holdout and Cross Validation. The idea behind both is that the training data can provide valuable insights over its distribution that we can use for both learning the right parameters and assessing our general model performance. \n",
    "\n",
    "In the case of Holdout, the training set is split into training and test data, with a usual ratio of 80:20. Once the learning algorithm computes its prediction parameters, we assess how well this model behaves on the test, unseen data - it is important that these two sets are disjoint, as we do not wish to train our model using testing datapoints. Note that the training set is further split into two: the actual training set and the validation set. The validation set is used to tune the hyperparameters, model parameters that the learning algorithm does not seek as that is usually too expensive to compute. However, the validation set only has a role during training for hyperparameters optimisation and thus underestimates the actual generalization error - that is the role of the testing set, where we evaluate how our model generalizes once the model parameters are tuned.\n",
    "\n",
    "However, the Holdout method becomes problematic when the available dataset is small. Usually, a small dataset is unlikely to represent the actual underlying distribution of data and, moreover, splitting it and using the test data only after the training is done might lead to a poor model. The solution for a better mean test error when the dataset is too small comes with the price of a higher computational cost. The idea behind methods such as k-fold Cross Validation is randomly generating a number of k different partitions of our dataset. Usually, the whole dataset is partitioned into k disjoint sets and, at iteration i, the ith subset of data becomes test data, while the other k - 1 are used for training, as done in Holdout. We repeat the training on each of these k different splits and average the test error over each partition. This solution aims to improve on the uncertainty over the estimated average test error.\n",
    "\n",
    "Deep Learning models are specifically efficient for problems where large datasets are collected. Thus, the Holdout method is more commonly used, as we tend to believe that large sets of data might represent the actual underlying distributions pretty well. Also, as the sets sizes increase, using Cross Validation might add expensive computations, slowing down the training process considerably, so Holdout remains the better solution for Deep Learning applications.\n",
    "\n",
    "# Solutions to overfitting\n",
    "\n",
    "L1, L2 Regularization - used to penalize big parameter values by adding a regularization term, i.e. from $\\min L(\\theta)$ to $\\min L(\\theta) + \\beta R(\\theta)$. From a statistical point of view, this is the same as adding a prior on the parameters. L2 ($|\\theta|^2$) has the advantage of penalizing influential features less than the others, while the L1 ($|\\theta|$) provides a sparse parameter space selection, i.e. lots of parameters will converge to 0.\n",
    "\n",
    "Early Stop - the idea is to consider the number of training steps as another hyperparameter to be tuned. This technique tackles the problem of specializing the model too much on the training data by saving the parameters that obtain the lowest validation set error (and, hopefully, a good enough test error). During training, we store the parameters for every iteration that improves the error on the validation set and, when the training finishes, we return these parameters rather than those that were obtained last.\n",
    "\n",
    "Dropout - zeroes the output of a node in the Neural Network setting with a certain probability. Hidden layers try to approximate the generator function and tend to fit a mapping that is maybe too specific, overfitting the data. By applying different network settings, i.e. by keeping a node with a certain probability (usually around 80%), the model becomes simpler and redundant features are less likely to be learnt by the network.\n",
    "\n",
    "Data augmentation - depending on what the dataset actually contains, we might want to add more data for our model by applying some transformations on the already existing one. By training our model on more data, we hope to reduce the generalization error. This is specifically simple for classification problems - for a specific input $(x, y)$, we wish to apply a transformation function that keeps the label unchanged, i.e. $f(x) = x^{\\prime}$ in order to obtain fake data as $(x^{\\prime}, y)$. Object recognition is a good example where we could apply this technique, as we can apply geometric transformations on pixels (e.g. rotations, translations) while still preserving the actual objects encountered in a picture. Some intuition behind this is that our Network might now be less prone to specific, ungeneralizable features, such as some pixel in different pictures always having the same color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "1. Why can gradient-based learning be difficult when using the sigmoid or hyperbolic tangent functions as hidden unit activation functions in deep, fully-connected neural networks?\n",
    "2. Why is the issue that arises in the previous question less of an issue when using such functions as output unit activation functions, provided that an appropriate loss function is used?\n",
    "3. What would happen if you initialize all the weights to zero in a multi-layer fully-connected neural network and attempt to train your model using gradient descent? What would happen if you did the same thing for a logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 2 IN THIS CELL*\\*\n",
    "\n",
    "# 1. Sigmoid and tanh for hidden layers in Deep Fully Connected NNs\n",
    "\n",
    "The sigmoid and tanh activation functions used to be pretty common as non-linear functions in fully-connected layers, however they lead to an issue known as the vanishing gradient problem. That is, the initialized weights are too big, these can saturate and entirely stop learning during the backward pass - the training loss will stop decreasing. The two functions saturate when the argument is very positive/negative, meaning that the functions become very flat and insensitive to small changes in the input. This happens as the derivative of sigmoid $z = \\frac{1}{1 + e^{-x}}$ is $z (1 - z)$, while the derivative of $z = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ is $1 - z^2$. As $x$ becomes too large (in both negative and positive directions), the sigmoid function becomes either $0$ or $1$ (i.e., take the limit of $x$ to $+-\\infty$), thus its derivative becomes $0$. Same happens for tanh, as $\\lim z^2 = 1$ when $x$ goes to $+- \\infty$. Thus, from this stage all the gradients become zero due to the chain rule. The gradient becomes null (vanishes), and the backward pass will keep the weights unchanged - the network stops learning.\n",
    "\n",
    "# 2. Better as output unit activation functions\n",
    "\n",
    "The sigmoid function can be used as an output activation function when the log-likelihood loss function is used. Using this combination is better than using sigmoid for hidden layers, as it ensures there is always a strong gradient whenever the model has the wrong answer. The gradient when using this combination is proportional to $(1 - \\sigma (y_i \\omega^T x_i))$ - saturation occurs only when the model already has the right answer - when $y_i = 1$ and $\\omega^Tx_i$ is very positive, or $y_i = -1$ and $\\omega^Tx_i$ is very negative. For the case when $\\omega^T x_i$ has the wrong sign, the gradient is not shrinked at all and the algorithm corrects the weights quickly (see proof in the Deep Learning Book, page 183).\n",
    "\n",
    "# 3. Zero-initialized weights\n",
    "\n",
    "The problem with zero-initialized weights in Neural Networks settings is that the initial parameters will not break symmetry between different units. As explained in Goodfellow, page 301, \"If two hidden units with the same activation function are connected to the same inputs, then these units must have different initial parameters\". Having same parameters will result in  an exact similar behaviour for different units, making the algorithm deterministic in terms of what parameters will be learnt at the end. Thus, having each unit compute different functions requires a random initialization of the weights during the first training iteration.\n",
    "\n",
    "This is not the case for logistic regression. First of all, the logistic regression is, by definition, only one function, so there are no other units that might emulate the same results. Also, in the case of logistic regression, zero-initialized weights ensure that the sigmoid values will stay within the linear area, thus ensuring a good propagation and a non-zero gradient. The gradient-based learning solution will eventually reach a local optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "In this part, you will use PyTorch to implement and train a multinomial logistic regression model to classify MNIST digits.\n",
    "\n",
    "Restrictions:\n",
    "* You must use (but not modify) the code provided in `utils.py`. **This file is deliberately not documented**; read it carefully as you will need to understand what it does to complete the tasks.\n",
    "* You are NOT allowed to use the `torch.nn` module.\n",
    "\n",
    "Please insert your solutions to the following tasks in the cells below:\n",
    "1. Complete the `MultinomialLogisticRegressionClassifier` class below by filling in the missing parts (expected behaviour is prescribed in the documentation):\n",
    "    * The constructor\n",
    "    * `forward`\n",
    "    * `parameters`\n",
    "    * `l1_weight_penalty`\n",
    "    * `l2_weight_penalty`\n",
    "\n",
    "2. The default hyperparameters for `MultilayerClassifier` and `run_experiment` have been deliberately chosen to produce poor results. Experiment with different hyperparameters until you are able to get a test set accuracy above 92% after a maximum of 10 epochs of training. However, DO NOT use the test set accuracy to tune your hyperparameters; use the validation loss / accuracy. You can use any optimizer in `torch.optim`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def get_layer_params(in_features, out_features, weight_init_sd=1.0):\n",
    "    nd = torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([weight_init_sd]))\n",
    "    params = (nd.sample(sample_shape=torch.Size([in_features, out_features]))).squeeze()\n",
    "    params.requires_grad_()\n",
    "    return params\n",
    "\n",
    "def add_bias(inputs):\n",
    "    return torch.cat([inputs, torch.ones([inputs.shape[0], 1])], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 3.1 IN THIS CELL*\n",
    "\n",
    "class MultinomialLogisticRegressionClassifier():\n",
    "    def __init__(self, weight_init_sd=1.0):\n",
    "        \"\"\"\n",
    "        Initializes model parameters to values drawn from the Normal\n",
    "        distribution with mean 0 and standard deviation `weight_init_sd`.\n",
    "        \"\"\"\n",
    "        self.weight_init_sd = weight_init_sd\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self.params = get_layer_params(785, 10, weight_init_sd=weight_init_sd)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "                \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the model.\n",
    "        \n",
    "        Expects `inputs` to be a Tensor of shape (batch_size, 1, 28, 28) containing\n",
    "        minibatch of MNIST images.\n",
    "        \n",
    "        Inputs should be flattened into a Tensor of shape (batch_size, 784),\n",
    "        before being fed into the model.\n",
    "        \n",
    "        Should return a Tensor of logits of shape (batch_size, 10).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **        \n",
    "        #######################################################################    \n",
    "        h = inputs.view(-1, 784)\n",
    "        h = add_bias(h)\n",
    "        return F.log_softmax(h @ self.params, dim=1)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Should return an iterable of all the model parameter Tensors.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return [self.params]\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "    def l1_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L1 norm of the model's weight vector (i.e. sum\n",
    "        of absolute values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return torch.sum(torch.sum(torch.abs(self.params), dim=1), dim=0)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def l2_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L2 weight penalty (i.e. \n",
    "        sum of squared values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return torch.sum(torch.sum(self.params ** 2, dim=1), dim=0)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:  0.00305 , l2 coefficient:  0.001  l1 coefficient:  0.0\n",
      "Epoch 0: training...\n",
      "Train set:\tAverage loss: 6.2320, Accuracy: 0.6592\n",
      "Validation set:\tAverage loss: 2.1162, Accuracy: 0.8187\n",
      "\n",
      "Epoch 1: training...\n",
      "Train set:\tAverage loss: 1.5808, Accuracy: 0.8463\n",
      "Validation set:\tAverage loss: 1.2596, Accuracy: 0.8650\n",
      "\n",
      "Epoch 2: training...\n",
      "Train set:\tAverage loss: 1.0272, Accuracy: 0.8686\n",
      "Validation set:\tAverage loss: 0.8726, Accuracy: 0.8765\n",
      "\n",
      "Epoch 3: training...\n",
      "Train set:\tAverage loss: 0.7296, Accuracy: 0.8781\n",
      "Validation set:\tAverage loss: 0.6847, Accuracy: 0.8725\n",
      "\n",
      "Epoch 4: training...\n",
      "Train set:\tAverage loss: 0.5596, Accuracy: 0.8850\n",
      "Validation set:\tAverage loss: 0.5240, Accuracy: 0.8838\n",
      "\n",
      "Epoch 5: training...\n",
      "Train set:\tAverage loss: 0.4505, Accuracy: 0.8911\n",
      "Validation set:\tAverage loss: 0.4714, Accuracy: 0.8892\n",
      "\n",
      "Epoch 6: training...\n",
      "Train set:\tAverage loss: 0.3859, Accuracy: 0.8989\n",
      "Validation set:\tAverage loss: 0.4146, Accuracy: 0.8930\n",
      "\n",
      "Epoch 7: training...\n",
      "Train set:\tAverage loss: 0.3511, Accuracy: 0.9037\n",
      "Validation set:\tAverage loss: 0.3730, Accuracy: 0.9015\n",
      "\n",
      "Epoch 8: training...\n",
      "Train set:\tAverage loss: 0.3373, Accuracy: 0.9052\n",
      "Validation set:\tAverage loss: 0.3520, Accuracy: 0.9018\n",
      "\n",
      "Epoch 9: training...\n",
      "Train set:\tAverage loss: 0.3269, Accuracy: 0.9079\n",
      "Validation set:\tAverage loss: 0.3388, Accuracy: 0.9083\n",
      "\n",
      "Test set:\tAverage loss: 0.3135, Accuracy: 0.9152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 3.2 IN THIS CELL - EXAMPLE WITH DEFAULT PARAMETERS PROVIDED *\n",
    "def run_with_model(lr=0.003, l1=0., l2=0.001, suppress_output=False, epochs=10, weight_init_sd=1.0):\n",
    "    print(\"learning rate: \", lr, \", l2 coefficient: \", l2, \" l1 coefficient: \", l1)\n",
    "    model = MultinomialLogisticRegressionClassifier(weight_init_sd=weight_init_sd)\n",
    "    res = run_experiment(\n",
    "        model,\n",
    "        optimizer=optim.Adam(model.parameters(), lr=lr),\n",
    "        train_loader=train_loader_1,\n",
    "        val_loader=val_loader_1,\n",
    "        test_loader=test_loader_1,\n",
    "        n_epochs=epochs,\n",
    "        l1_penalty_coef=l1,\n",
    "        l2_penalty_coef=l2,\n",
    "        suppress_output=suppress_output\n",
    "    )\n",
    "    return res\n",
    "\n",
    "# lr: 0.003, l2: 0.001, weight: 1.0, acc: 0.915\n",
    "# Test set:\tAverage loss: 0.3061, Accuracy: 0.9160\n",
    "\n",
    "lr = 0.00305\n",
    "l1 = 0.0\n",
    "l2 = 0.001\n",
    "weight = 1.0\n",
    "epochs = 10\n",
    "\n",
    "res = run_with_model(lr=lr, l2=l2, l1=l1, epochs=epochs, weight_init_sd=weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "In this part, you will use PyTorch to implement and train a multi-layer fully-connected neural network to classify MNIST digits.\n",
    "\n",
    "Your network must have three hidden layers with 128, 64, and 32 hidden units respectively.\n",
    "\n",
    "The same restrictions as in Part 3 apply.\n",
    "\n",
    "Please insert your solutions to the following tasks in the cells below:\n",
    "1. Complete the `MultilayerClassifier` class below by filling in the missing parts of the following methods (expected behaviour is prescribed in the documentation):\n",
    "\n",
    "    * The constructor\n",
    "    * `forward`\n",
    "    * `parameters`\n",
    "    * `l1_weight_penalty`\n",
    "    * `l2_weight_penalty`\n",
    "\n",
    "2. The default hyperparameters for `MultilayerClassifier` and `run_experiment` have been deliberately chosen to produce poor results. Experiment with different hyperparameters until you are able to get a test set accuracy above 97% after a maximum of 10 epochs of training. However, DO NOT use the test set accuracy to tune your hyperparameters; use the validation loss / accuracy. You can use any optimizer in `torch.optim`.\n",
    "\n",
    "3. Describe an alternative strategy for initializing weights that may perform better than the strategy we have used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 4.1 IN THIS CELL*\n",
    "class MultilayerClassifier:\n",
    "    def __init__(self, activation_fun=\"sigmoid\", weight_init_sd=1.0):\n",
    "        \"\"\"\n",
    "        Initializes model parameters to values drawn from the Normal\n",
    "        distribution with mean 0 and standard deviation `weight_init_sd`.\n",
    "        \"\"\"\n",
    "        self.activation_fun = activation_fun\n",
    "        self.weight_init_sd = weight_init_sd\n",
    "\n",
    "        if self.activation_fun == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif self.activation_fun == \"sigmoid\":\n",
    "            self.activation = torch.sigmoid\n",
    "        elif self.activation_fun == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        params_1 = get_layer_params(785, 128, weight_init_sd=weight_init_sd)\n",
    "        params_2 = get_layer_params(129, 64, weight_init_sd=weight_init_sd)\n",
    "        params_3 = get_layer_params(65, 32, weight_init_sd=weight_init_sd)\n",
    "        params_4 = get_layer_params(33, 10, weight_init_sd=weight_init_sd)        \n",
    "                \n",
    "        self.params = [params_1, params_2, params_3, params_4]\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the model.\n",
    "        \n",
    "        Expects `inputs` to be Tensor of shape (batch_size, 1, 28, 28) containing\n",
    "        minibatch of MNIST images.\n",
    "        \n",
    "        Inputs should be flattened into a Tensor of shape (batch_size, 784),\n",
    "        before being fed into the model.\n",
    "        \n",
    "        Should return a Tensor of logits of shape (batch_size, 10).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        h = inputs.view(-1, 784)\n",
    "                \n",
    "        for indx, param in enumerate(self.parameters()):\n",
    "            if indx == len(self.parameters()) - 1:\n",
    "                break\n",
    "            h = add_bias(h)\n",
    "            h = self.activation(h @ param)\n",
    "        \n",
    "        h = add_bias(h)\n",
    "        return F.log_softmax(h @ self.parameters()[3], dim=1)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Should return an iterable of all the model parameter Tensors.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return self.params\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "    \n",
    "    def l1_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L1 norm of the model's weight vector (i.e. sum\n",
    "        of absolute values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        S = 0.\n",
    "        \n",
    "        for param in self.params:\n",
    "            S += torch.sum(torch.sum(torch.abs(param), dim=1), dim=0)\n",
    "        return S\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def l2_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L2 weight penalty (i.e. \n",
    "        sum of squared values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        S = 0.\n",
    "        for param in self.params:\n",
    "            S += torch.sum(torch.sum(param ** 2, dim=1), dim=0)\n",
    "        return S    \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training...\n",
      "Train set:\tAverage loss: 0.2800, Accuracy: 0.9144\n",
      "Validation set:\tAverage loss: 0.2051, Accuracy: 0.9382\n",
      "\n",
      "Epoch 1: training...\n",
      "Train set:\tAverage loss: 0.1815, Accuracy: 0.9457\n",
      "Validation set:\tAverage loss: 0.1681, Accuracy: 0.9505\n",
      "\n",
      "Epoch 2: training...\n",
      "Train set:\tAverage loss: 0.1669, Accuracy: 0.9493\n",
      "Validation set:\tAverage loss: 0.1588, Accuracy: 0.9498\n",
      "\n",
      "Epoch 3: training...\n",
      "Train set:\tAverage loss: 0.1587, Accuracy: 0.9520\n",
      "Validation set:\tAverage loss: 0.1591, Accuracy: 0.9530\n",
      "\n",
      "Epoch 4: training...\n",
      "Train set:\tAverage loss: 0.1527, Accuracy: 0.9529\n",
      "Validation set:\tAverage loss: 0.1584, Accuracy: 0.9538\n",
      "\n",
      "Epoch 5: training...\n",
      "Train set:\tAverage loss: 0.1486, Accuracy: 0.9549\n",
      "Validation set:\tAverage loss: 0.1590, Accuracy: 0.9525\n",
      "\n",
      "Epoch 6: training...\n",
      "Train set:\tAverage loss: 0.1471, Accuracy: 0.9553\n",
      "Validation set:\tAverage loss: 0.1595, Accuracy: 0.9535\n",
      "\n",
      "Epoch 7: training...\n",
      "Train set:\tAverage loss: 0.1480, Accuracy: 0.9557\n",
      "Validation set:\tAverage loss: 0.1880, Accuracy: 0.9413\n",
      "\n",
      "Epoch 8: training...\n",
      "Train set:\tAverage loss: 0.1429, Accuracy: 0.9570\n",
      "Validation set:\tAverage loss: 0.1413, Accuracy: 0.9583\n",
      "\n",
      "Epoch 9: training...\n",
      "Train set:\tAverage loss: 0.1445, Accuracy: 0.9562\n",
      "Validation set:\tAverage loss: 0.1761, Accuracy: 0.9463\n",
      "\n",
      "Test set:\tAverage loss: 0.1488, Accuracy: 0.9555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 4.2 IN THIS CELL - EXAMPLE WITH DEFAULT PARAMETERS PROVIDED *\n",
    "\n",
    "# lr = 0.0043\n",
    "# l1 = 0.0\n",
    "# l2 = 0.001\n",
    "# weight = 1.0\n",
    "# func = 'relu'\n",
    "# Test set:\tAverage loss: 0.1473, Accuracy: 0.9572\n",
    "\n",
    "lr = 0.0043\n",
    "l1 = 0.0\n",
    "l2 = 0.001\n",
    "weight = 1.0\n",
    "func = 'relu'\n",
    "# Test set:\tAverage loss: 0.1191, Accuracy: 0.9637\n",
    "\n",
    "model = MultilayerClassifier(activation_fun=func, weight_init_sd=weight)\n",
    "res = run_experiment(\n",
    "    model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr),\n",
    "    train_loader=train_loader_0,\n",
    "    val_loader=val_loader_0,\n",
    "    test_loader=test_loader_0,\n",
    "    n_epochs=10,\n",
    "    l1_penalty_coef=l1,\n",
    "    l2_penalty_coef=l2,\n",
    "    suppress_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 4.3 IN THIS CELL*\\*\n",
    "\n",
    "An alternative to what we have used so far for weight initialization would be to use the He or Xavier initialization strategies. Instead of using the same weight variance for all parameters, we could make use of the layers' number of nodes to fill in the variance. The He initialization uses as variance $\\frac{1}{2 \\cdot n_{\\text{in}}}$, while the Xavier initialization uses as variance $\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}$. Considering our setting, the He initialization could provide better results, as it is apparently better for ReLu settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
